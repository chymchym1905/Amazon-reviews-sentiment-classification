{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import gzip\n",
    "# import numpy as np\n",
    "# # import json\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Video_Games_Resampled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.reindex(np.random.permutation(df.index))\n",
    "# ratingonly = df.shape\n",
    "# df = df.dropna(subset=['overall'],inplace=True)\n",
    "# df = df.dropna(subset=['reviewText'],inplace=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax=sns.countplot(x='overall', data=df)\n",
    "# ax.bar_label(ax.containers[0])\n",
    "# plt.grid(visible=True,axis='y',alpha=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating_threshold = 4\n",
    "# df['Positive Rating'] = df['overall'].apply(lambda x: 1 if x>=rating_threshold else 0)\n",
    "# df['Positive Rating'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# import re\n",
    "# from nltk.tokenize import word_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(text):\n",
    "#     # lower text\n",
    "#     text = text.lower()\n",
    "#     # remove hyperlinks\n",
    "#     text = re.compile(r'^https?://', re.IGNORECASE).sub(r'', text)\n",
    "#     # remove non-letters\n",
    "#     text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "#     text = word_tokenize(text)\n",
    "#     # remove stopwords\n",
    "#     stops = set(stopwords.words(\"english\"))\n",
    "#     text = [w for w in text if not w in stops]\n",
    "#     text = \" \".join(text)\n",
    "#     # remove punctuation\n",
    "#     text = RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "#     text = \" \".join(text)\n",
    "#     # remove words less than 3 letters\n",
    "#     text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "#     return text  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dropna(subset=['reviewText'],inplace=True)\n",
    "# df['reviewText'] = df['reviewText'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "\n",
    "# majority = df[df['Positive Rating']==1]\n",
    "# minority = df[df['Positive Rating']==0]\n",
    "# minority_upsampled = resample(minority, replace=True, n_samples=int(len(minority)*1.6), random_state=87)\n",
    "# majority_downsampled =  resample(majority, replace=True, n_samples=int(len(majority)/2.2), random_state=100)\n",
    "# resampled_df = pd.concat([majority_downsampled, minority_upsampled])\n",
    "\n",
    "# print(resampled_df.shape)\n",
    "# resampled_df['overall'].value_counts()\n",
    "# resampled_df.to_csv('Video_Games_Resampled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv('Video_Games_Resampled.csv')\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainx, testx, trainy, testy = train_test_split(df['reviewText'], df['Positive Rating'], test_size=0.23, random_state=42)\n",
    "print(len(trainx))\n",
    "print(len(testx))\n",
    "train = pd.concat([trainx, trainy], axis =1)\n",
    "test = pd.concat([testx, testy], axis=1)\n",
    "# train.to_csv('train_data.csv', index=True)\n",
    "# test.to_csv('test_data.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "eng = spacy.load(\"en_core_web_sm\")\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "def engTokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize an English text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in eng.tokenizer(text)]\n",
    "\n",
    "def tokenize(data):\n",
    "    for x in data:\n",
    "        yield engTokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_train = build_vocab_from_iterator(\n",
    "    tokenize(trainx),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "vocab_train.set_default_index(vocab_train['<unk>'])\n",
    "vocab_test = build_vocab_from_iterator(\n",
    "    tokenize(testx),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "vocab_test.set_default_index(vocab_test['<unk>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# if os.path.exists('vocab_train.pkl'):\n",
    "#     print('Load vocab train')\n",
    "#     with open('vocab_train.pkl', 'rb') as vocab_train_file:\n",
    "#         vocab_train = pickle.load(vocab_train_file)\n",
    "# else:\n",
    "#     # Save the trained vocab_train\n",
    "#     print('Save vocab train')\n",
    "#     with open('vocab_train.pkl', 'wb') as vocab_train_file:\n",
    "#         pickle.dump(vocab_train, vocab_train_file)\n",
    "\n",
    "# if os.path.exists('vocab_test.pkl'):\n",
    "#     print('Load vocab test')\n",
    "#     # Load vocab_test from the pickle file\n",
    "#     with open('vocab_test.pkl', 'rb') as vocab_test_file:\n",
    "#         vocab_test = pickle.load(vocab_test_file)\n",
    "# else:\n",
    "#     print('Save vocab test')\n",
    "#     # Save the trained vocab_test\n",
    "#     with open('vocab_test.pkl', 'wb') as vocab_test_file:\n",
    "#         pickle.dump(vocab_test, vocab_test_file)\n",
    "\n",
    "\n",
    "# print(vocab_train.get_itos()[:45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabsize = len(vocab_train)\n",
    "print(vocabsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions\n",
    "input_dim = vocabsize # The dimension of your input data (e.g., vocabulary size)\n",
    "hidden_dim = 256  # Size of the hidden layer\n",
    "embedding_dim =  128\n",
    "output_dim = 2  # Two classes: positive and negative\n",
    "learning_rate = 0.001\n",
    "epochs = 15\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classification model\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        # self.rnn = torch.nn.RNN(embedding_dim,\n",
    "        #                        hidden_dim,\n",
    "        #                        nonlinearity='relu')\n",
    "        self.rnn = torch.nn.LSTM(input_size= embedding_dim,\n",
    "                                 hidden_size=hidden_dim)        \n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text, length):\n",
    "        # text dim: [sentence length, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        # embedded dim: [sentence length, batch size, embedding dim]\n",
    "        # Pack the sequences for efficient processing\n",
    "        packed_data = pack_padded_sequence(embedded, length.sum(1), batch_first=True, enforce_sorted=False)\n",
    "        output, (hidden, cell) = self.rnn(packed_data)\n",
    "        # output dim: [sentence length, batch size, hidden dim]\n",
    "        # hidden dim: [1, batch size, hidden dim]\n",
    "        unpacked_output, unpacked_length = pad_packed_sequence(output,batch_first=True)\n",
    "        unpacked_output.squeeze_(1)\n",
    "        # hidden dim: [batch size, hidden dim]\n",
    "        o = self.fc(unpacked_output)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # sample = {'data': self.data[idx], 'label': self.labels[idx]}\n",
    "        return self.data.iloc[idx], self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = trainy.values\n",
    "y_test = testy.values\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransform(vocab):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    text_tranform = T.Sequential(\n",
    "        ## converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "        # 2 as seen in previous section\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_tranform\n",
    "def applyTransform(text):\n",
    "    \"\"\"\n",
    "    Apply transforms to sequence of tokens in a sequence pair\n",
    "    \"\"\"\n",
    "\n",
    "    return getTransform(vocab_train)(engTokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = trainx.apply(applyTransform)\n",
    "temp = list(x_train)\n",
    "print(x_train[3])\n",
    "x_test = testx.apply(applyTransform)\n",
    "temp=list(x_test)\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.set_device(0)\n",
    "    print(\"GPU available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available. Using CPU.\")\n",
    "print(device)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    data, labels = zip(*batch)\n",
    "\n",
    "    # Sort sequences by length (from longest to shortest)\n",
    "    sorted_data, sorted_labels = zip(*sorted(zip(data, labels), key=lambda x: len(x[0]), reverse=True))\n",
    "\n",
    "    # Pad sequences to the length of the longest sequence\n",
    "    padded_data = pad_sequence([torch.tensor(seq) for seq in sorted_data], batch_first=True)\n",
    "    padded_labels = torch.tensor(sorted_labels, dtype=torch.long)\n",
    "\n",
    "    # Create a mask for the padded elements\n",
    "    mask = (padded_data != 0).float()\n",
    "\n",
    "    return padded_data.to(device), mask, padded_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CustomDataset(x_train, y_train)\n",
    "testset = CustomDataset(x_test, y_test)\n",
    "\n",
    "# trainset = CustomDataset(vocab_train, y_train_onehot)\n",
    "# testset = CustomDataset(vocab_test, y_test)\n",
    "\n",
    "# Create a DataLoader for your dataset\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Move your model to the GPU\n",
    "\n",
    "# Instantiate the classifier\n",
    "model = SentimentClassifier(input_dim, embedding_dim, hidden_dim, output_dim).to(device)\n",
    "# Define the loss function (cross-entropy) and the optimizer (Adam)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,y,z in train_loader:\n",
    "#     print(x)\n",
    "#     print('AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')\n",
    "#     print(y)\n",
    "#     print('AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')\n",
    "#     print(z)\n",
    "#     print('AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')\n",
    "#     # seq_unpacked, lens_unpacked = pad_packed_sequence(x, batch_first=True)\n",
    "#     # print(seq_unpacked)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Wrap train_loader with tqdm\n",
    "with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Training\")\n",
    "        model.train()\n",
    "        # Wrap train_loader with tqdm to display progress bar\n",
    "        with tqdm(tepoch, unit=\"batch\") as tbatch:\n",
    "            for packed_data, mask, labels in tbatch:\n",
    "                labels = labels.unsqueeze(1)\n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(packed_data, mask)\n",
    "                loss = criterion(predictions, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tbatch.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Evaluation on the test set\n",
    "        print(\"Testing\")\n",
    "        model.eval()\n",
    "        total_acc, total_count = 0, 0\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Wrap test_loader with tqdm to display progress bar\n",
    "            with tqdm(test_loader, unit=\"batch\") as ttest:\n",
    "                for packed_data, mask, labels in ttest:\n",
    "                    labels = labels.unsqueeze(1)\n",
    "                    predictions = model(packed_data, mask)\n",
    "                    result = predictions.argmax(1)==z\n",
    "                    result = result[torch.all(result, dim=1)]\n",
    "                    total_acc += len(result)\n",
    "                    total_count += len(z)\n",
    "                    ttest.set_postfix(accuracy=total_acc / total_count)\n",
    "\n",
    "        accuracy = total_acc / total_count\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] | Test Accuracy: {accuracy:.2%}')\n",
    "\n",
    "# Print total training time\n",
    "print(\"Total training time: {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
