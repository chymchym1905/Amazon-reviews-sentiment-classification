{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import gzip\n",
    "import numpy as np\n",
    "# import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Video_Games_Resampled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "      <th>Positive Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>346407</td>\n",
       "      <td>129412</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>05 3, 2010</td>\n",
       "      <td>A3DID3K8W8QD9W</td>\n",
       "      <td>B0012LHO46</td>\n",
       "      <td>Sam Hartford</td>\n",
       "      <td>bought mine ebay one amazon work fine generics...</td>\n",
       "      <td>PS2 64mb Memory Card is A-OK</td>\n",
       "      <td>1272844800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19478</td>\n",
       "      <td>204432</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>05 5, 2014</td>\n",
       "      <td>A1SPRD853KYUP1</td>\n",
       "      <td>B002I0K956</td>\n",
       "      <td>Oswaldo Romero</td>\n",
       "      <td>nice</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>1399248000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>349043</td>\n",
       "      <td>367307</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>01 4, 2017</td>\n",
       "      <td>A4SV916591F5Z</td>\n",
       "      <td>B00KY1I0IO</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>great fun addicting</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1483488000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Format:': ' Video Game'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>238716</td>\n",
       "      <td>381621</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 12, 2016</td>\n",
       "      <td>AWXOF97NP6ZZG</td>\n",
       "      <td>B00O9GPD26</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>surprisingly higher quality figure american ve...</td>\n",
       "      <td>The figure is better than the american version.</td>\n",
       "      <td>1455235200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>416659</td>\n",
       "      <td>456490</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>08 5, 2005</td>\n",
       "      <td>A3LOOGDJ7S01LA</td>\n",
       "      <td>B00008J2V0</td>\n",
       "      <td>whiterabbit</td>\n",
       "      <td>highly recommended whole family easy instructi...</td>\n",
       "      <td>Lots of Fun, Lots to Explore!</td>\n",
       "      <td>1123200000</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  overall  verified   reviewTime      reviewerID  \\\n",
       "0        346407      129412      5.0     False   05 3, 2010  A3DID3K8W8QD9W   \n",
       "1         19478      204432      4.0      True   05 5, 2014  A1SPRD853KYUP1   \n",
       "2        349043      367307      5.0      True   01 4, 2017   A4SV916591F5Z   \n",
       "3        238716      381621      4.0      True  02 12, 2016   AWXOF97NP6ZZG   \n",
       "4        416659      456490      5.0     False   08 5, 2005  A3LOOGDJ7S01LA   \n",
       "\n",
       "         asin     reviewerName  \\\n",
       "0  B0012LHO46     Sam Hartford   \n",
       "1  B002I0K956   Oswaldo Romero   \n",
       "2  B00KY1I0IO  Amazon Customer   \n",
       "3  B00O9GPD26  Amazon Customer   \n",
       "4  B00008J2V0      whiterabbit   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  bought mine ebay one amazon work fine generics...   \n",
       "1                                               nice   \n",
       "2                                great fun addicting   \n",
       "3  surprisingly higher quality figure american ve...   \n",
       "4  highly recommended whole family easy instructi...   \n",
       "\n",
       "                                           summary  unixReviewTime vote  \\\n",
       "0                     PS2 64mb Memory Card is A-OK      1272844800  NaN   \n",
       "1                                       Four Stars      1399248000  NaN   \n",
       "2                                       Five Stars      1483488000  NaN   \n",
       "3  The figure is better than the american version.      1455235200  NaN   \n",
       "4                    Lots of Fun, Lots to Explore!      1123200000    7   \n",
       "\n",
       "                        style image  Positive Rating  \n",
       "0                         NaN   NaN                1  \n",
       "1                         NaN   NaN                1  \n",
       "2  {'Format:': ' Video Game'}   NaN                1  \n",
       "3                         NaN   NaN                1  \n",
       "4                         NaN   NaN                1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df.reindex(np.random.permutation(df.index))\n",
    "# ratingonly = df.shape\n",
    "# df = df.dropna(subset=['overall'],inplace=True)\n",
    "# df = df.dropna(subset=['reviewText'],inplace=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax=sns.countplot(x='overall', data=df)\n",
    "# ax.bar_label(ax.containers[0])\n",
    "# plt.grid(visible=True,axis='y',alpha=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating_threshold = 4\n",
    "# df['Positive Rating'] = df['overall'].apply(lambda x: 1 if x>=rating_threshold else 0)\n",
    "# df['Positive Rating'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# import re\n",
    "# from nltk.tokenize import word_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(text):\n",
    "#     # lower text\n",
    "#     text = text.lower()\n",
    "#     # remove hyperlinks\n",
    "#     text = re.compile(r'^https?://', re.IGNORECASE).sub(r'', text)\n",
    "#     # remove non-letters\n",
    "#     text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "#     text = word_tokenize(text)\n",
    "#     # remove stopwords\n",
    "#     stops = set(stopwords.words(\"english\"))\n",
    "#     text = [w for w in text if not w in stops]\n",
    "#     text = \" \".join(text)\n",
    "#     # remove punctuation\n",
    "#     text = RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "#     text = \" \".join(text)\n",
    "#     # remove words less than 3 letters\n",
    "#     text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "#     return text  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dropna(subset=['reviewText'],inplace=True)\n",
    "# df['reviewText'] = df['reviewText'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "\n",
    "# majority = df[df['Positive Rating']==1]\n",
    "# minority = df[df['Positive Rating']==0]\n",
    "# minority_upsampled = resample(minority, replace=True, n_samples=int(len(minority)*1.6), random_state=87)\n",
    "# majority_downsampled =  resample(majority, replace=True, n_samples=int(len(majority)/2.2), random_state=100)\n",
    "# resampled_df = pd.concat([majority_downsampled, minority_upsampled])\n",
    "\n",
    "# print(resampled_df.shape)\n",
    "# resampled_df['overall'].value_counts()\n",
    "# resampled_df.to_csv('Video_Games_Resampled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv('Video_Games_Resampled.csv')\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263969\n",
      "78849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainx, testx, trainy, testy = train_test_split(df['reviewText'], df['Positive Rating'], test_size=0.23, random_state=42)\n",
    "print(len(trainx))\n",
    "print(len(testx))\n",
    "train = pd.concat([trainx, trainy], axis =1)\n",
    "test = pd.concat([testx, testy], axis=1)\n",
    "# train.to_csv('train_data.csv', index=True)\n",
    "# test.to_csv('test_data.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "# spacy.cli.download(\"en_core_web_sm\")##newline\n",
    "eng = spacy.load(\"en_core_web_sm\")\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "def engTokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize an English text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in eng.tokenizer(text)]\n",
    "\n",
    "def tokenize(data):\n",
    "    for x in data:\n",
    "        yield engTokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_train = build_vocab_from_iterator(\n",
    "#     tokenize(trainx),\n",
    "#     min_freq=2,\n",
    "#     specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "#     special_first=True\n",
    "# )\n",
    "# vocab_train.set_default_index(vocab_train['<unk>'])\n",
    "# vocab_test = build_vocab_from_iterator(\n",
    "#     tokenize(testx),\n",
    "#     min_freq=2,\n",
    "#     specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "#     special_first=True\n",
    "# )\n",
    "# vocab_test.set_default_index(vocab_test['<unk>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load vocab train\n",
      "Load vocab test\n",
      "['<pad>', '<sos>', '<eos>', '<unk>', 'game', ' ', 'like', 'one', 'games', 'get', 'play', 'good', 'really', 'time', 'great', 'fun', 'would', 'much', 'even', 'first', 'also', 'well', 'graphics', 'story', 'new', 'playing', 'way', 'better', 'still', 'make', 'use', 'played', 'many', 'could', 'lot', 'want', 'back', 'little', 'pretty', 'buy', 'gameplay', 'bad', 'people', 'best', 'got']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "if os.path.exists('vocab_train.pkl'):\n",
    "    print('Load vocab train')\n",
    "    with open('vocab_train.pkl', 'rb') as vocab_train_file:\n",
    "        vocab_train = pickle.load(vocab_train_file)\n",
    "# else:\n",
    "#     # Save the trained vocab_train\n",
    "#     print('Save vocab train')\n",
    "#     with open('vocab_train.pkl', 'wb') as vocab_train_file:\n",
    "#         pickle.dump(vocab_train, vocab_train_file)\n",
    "\n",
    "if os.path.exists('vocab_test.pkl'):\n",
    "    print('Load vocab test')\n",
    "    # Load vocab_test from the pickle file\n",
    "    with open('vocab_test.pkl', 'rb') as vocab_test_file:\n",
    "        vocab_test = pickle.load(vocab_test_file)\n",
    "# else:\n",
    "#     print('Save vocab test')\n",
    "#     # Save the trained vocab_test\n",
    "#     with open('vocab_test.pkl', 'wb') as vocab_test_file:\n",
    "#         pickle.dump(vocab_test, vocab_test_file)\n",
    "\n",
    "\n",
    "print(vocab_train.get_itos()[:45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77587\n"
     ]
    }
   ],
   "source": [
    "vocabsize = len(vocab_train)\n",
    "print(vocabsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions\n",
    "input_dim = vocabsize # The dimension of your input data (e.g., vocabulary size)\n",
    "hidden_dim = 256  # Size of the hidden layer\n",
    "embedding_dim =  128\n",
    "output_dim = 1  # Two classes: positive and negative\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available. Using GPU.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available. Using CPU.\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "# Define the classification model\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        # self.rnn = torch.nn.RNN(embedding_dim,\n",
    "        #                        hidden_dim,\n",
    "        #                        nonlinearity='relu')\n",
    "        self.rnn = nn.LSTM(input_size= embedding_dim,\n",
    "                                hidden_size=hidden_dim)        \n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32,output_dim)\n",
    "        # self.sm = nn.LogSoftmax(dim=1)\n",
    "    #     self.hidden = self.init_hidden()\n",
    "\n",
    "    # def init_hidden(self):\n",
    "    #     # the first is the hidden h\n",
    "    #     # the second is the cell  c\n",
    "    #     return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim, dtype=torch.float)).to(device),\n",
    "    #             autograd.Variable(torch.zeros(1, 1, self.hidden_dim, dtype=torch.float)).to(device))\n",
    "    \n",
    "    def forward(self, text, length):\n",
    "        # text dim: [sentence length, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        # embedded dim: [sentence length, batch size, embedding dim]\n",
    "        # Pack the sequences for efficient processing\n",
    "        packed_data = pack_padded_sequence(embedded, length.sum(1), batch_first=True, enforce_sorted=False)\n",
    "        output, (hidden, cell) = self.rnn(packed_data)\n",
    "        # output dim: [sentence length, batch size, hidden dim]\n",
    "        # hidden dim: [1, batch size, hidden dim]\n",
    "        # unpacked_output, unpacked_length = pad_packed_sequence(output,batch_first=True)\n",
    "        # unpacked_output.squeeze_(1)\n",
    "        # hidden dim: [batch size, hidden dim]\n",
    "        o = self.fc1(hidden.squeeze(0))\n",
    "        o = self.dropout(o)\n",
    "        o = self.fc3(o)\n",
    "        o = self.dropout(o)\n",
    "        o = self.fc4(o)\n",
    "        # o = self.sm(o)\n",
    "        # o = self.sigmoid(o.squeeze(0))\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # sample = {'data': self.data[idx], 'label': self.labels[idx]}\n",
    "        return self.data.iloc[idx], self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_train = trainy.values\n",
    "y_test = testy.values\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransform(vocab):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    text_tranform = T.Sequential(\n",
    "        ## converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "        # 2 as seen in previous section\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_tranform\n",
    "def applyTransform(text):\n",
    "    \"\"\"\n",
    "    Apply transforms to sequence of tokens in a sequence pair\n",
    "    \"\"\"\n",
    "\n",
    "    return getTransform(vocab_train)(engTokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = trainx.apply(applyTransform)\n",
    "# x_test = testx.apply(applyTransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load x_train vectors\n",
      "Load x_test vectors\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('x_train.pkl'):\n",
    "    print('Load x_train vectors')\n",
    "    # Load vocab_test from the pickle file\n",
    "    with open('x_train.pkl', 'rb') as x_train_file:\n",
    "        x_train = pickle.load(x_train_file)\n",
    "# else:\n",
    "#     print('Save x_train vectors')\n",
    "#     # Save the trained vocab_test\n",
    "#     with open('x_train.pkl', 'wb') as x_train_file:\n",
    "#         pickle.dump(x_train, x_train_file)\n",
    "\n",
    "if os.path.exists('x_test.pkl'):\n",
    "    print('Load x_test vectors')\n",
    "    # Load vocab_test from the pickle file\n",
    "    with open('x_test.pkl', 'rb') as x_test_file:\n",
    "        x_test = pickle.load(x_test_file)\n",
    "# else:\n",
    "#     print('Save x_test vectors')\n",
    "#     # Save the trained vocab_test\n",
    "#     with open('x_test.pkl', 'wb') as x_test_file:\n",
    "#         pickle.dump(x_test, x_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    data, labels = zip(*batch)\n",
    "\n",
    "    # Sort sequences by length (from longest to shortest)\n",
    "    sorted_data, sorted_labels = zip(*sorted(zip(data, labels), key=lambda x: len(x[0]), reverse=True))\n",
    "\n",
    "    # Pad sequences to the length of the longest sequence\n",
    "    padded_data = pad_sequence([torch.tensor(seq) for seq in sorted_data], batch_first=True)\n",
    "    padded_labels = torch.tensor(sorted_labels, dtype=torch.float64)\n",
    "\n",
    "    # Create a mask for the padded elements\n",
    "    mask = (padded_data != 0).float()\n",
    "\n",
    "    return padded_data.to(device), mask, padded_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "trainset = CustomDataset(x_train, y_train)\n",
    "testset = CustomDataset(x_test, y_test)\n",
    "\n",
    "# trainset = CustomDataset(vocab_train, y_train_onehot)\n",
    "# testset = CustomDataset(vocab_test, y_test)\n",
    "\n",
    "# Create a DataLoader for your dataset\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier\n",
    "model = SentimentClassifier(input_dim, embedding_dim, hidden_dim, output_dim).to(device)\n",
    "# Define the loss function (cross-entropy) and the optimizer (Adam)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,   86, 9462,  ...,  624, 1187,    2],\n",
      "        [   1,    5,  688,  ...,    0,    0,    0],\n",
      "        [   1, 1129, 1089,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   1,  420,    2,  ...,    0,    0,    0],\n",
      "        [   1,  249,    2,  ...,    0,    0,    0],\n",
      "        [   1,   41,    2,  ...,    0,    0,    0]], device='cuda:0')\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]], device='cuda:0', dtype=torch.float64)\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n"
     ]
    }
   ],
   "source": [
    "for x,y,z in test_loader:\n",
    "    print(x)\n",
    "    print('AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')\n",
    "    print(y)\n",
    "    print('AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')\n",
    "    print(z.unsqueeze(1))\n",
    "    print('AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')\n",
    "    # seq_unpacked, lens_unpacked = pad_packed_sequence(x, batch_first=True)\n",
    "    # print(seq_unpacked)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]], device='cuda:0')\n",
      "38.28125%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(76)\n",
    "total_acc, total_count = 0,0\n",
    "for x,y,z in test_loader:\n",
    "    z = z.unsqueeze(1)\n",
    "    predictions = model(x,y)\n",
    "    # print(predictions)\n",
    "    # print(z)\n",
    "    loss = criterion(predictions,z)\n",
    "    loss.backward()\n",
    "    # print(loss.item())\n",
    "    \n",
    "    result = (predictions>0).float()\n",
    "    result = result[torch.all(result==z, dim=1)]\n",
    "    print(result)\n",
    "    total_acc += len(result)\n",
    "    total_count += len(z)\n",
    "    print(f\"{total_acc/total_count*100}%\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainloop(trainloader):\n",
    "    epoch_train_loss = 0.0\n",
    "    model.train()\n",
    "    with tqdm(trainloader, unit=\"batch\", desc='Training: ') as tbatch:\n",
    "        for data, mask, labels in tbatch:\n",
    "            labels = labels.unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data, mask)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss\n",
    "            tbatch.set_postfix(loss=\"{:.6f}\".format(loss))\n",
    "    return epoch_train_loss/len(trainloader)\n",
    "\n",
    "def evaluateloop(testloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with tqdm(testloader, unit=\"batch\", desc='Evaluating: ') as ttest:\n",
    "        with torch.no_grad():\n",
    "            for data, mask, labels in ttest:\n",
    "                labels = labels.unsqueeze(1)\n",
    "                predictions = model(data, mask)\n",
    "                result = (predictions>0).float()\n",
    "                result = result[torch.all(result==labels, dim=1)]\n",
    "                total_acc += len(result)\n",
    "                total_count += len(labels)\n",
    "                ttest.set_postfix({'accuracy': total_acc / total_count})\n",
    "    accuracy = total_acc / total_count\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(model, optimizer, loss, epoch, filename):\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, filename)\n",
    "def resume(filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2063 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2063/2063 [05:03<00:00,  6.79batch/s, loss=0.481353]\n",
      "Evaluating: 100%|██████████| 617/617 [00:37<00:00, 16.42batch/s, accuracy=0.83] \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mi:\\Bách Khoa\\CO3029\\mini project\\main.ipynb Cell 32\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/B%C3%A1ch%20Khoa/CO3029/mini%20project/main.ipynb#X43sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m loss_array\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m[train_loss]\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/B%C3%A1ch%20Khoa/CO3029/mini%20project/main.ipynb#X43sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mif\u001b[39;00m counter\u001b[39m%\u001b[39m\u001b[39m5\u001b[39m\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/i%3A/B%C3%A1ch%20Khoa/CO3029/mini%20project/main.ipynb#X43sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     checkpoint(model, optimizer, loss_array[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], curr_epoch, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mModel_at_epoch_\u001b[39m\u001b[39m{\u001b[39;00mcurr_epoch\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/B%C3%A1ch%20Khoa/CO3029/mini%20project/main.ipynb#X43sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mloss_values.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f_loss, \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39maccuracy_values.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f_accuracy:\n\u001b[0;32m     <a href='vscode-notebook-cell:/i%3A/B%C3%A1ch%20Khoa/CO3029/mini%20project/main.ipynb#X43sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         pickle\u001b[39m.\u001b[39mdump(loss_array, f_loss)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "torch.manual_seed(0)\n",
    "last_lost = 0.0 #the lost of the last epoch\n",
    "patience = 3 \n",
    "trigger_times = 0 #counter for early stop\n",
    "stop = False\n",
    "start_epoch=0\n",
    "curr_epoch = 0\n",
    "delta = 0.0002\n",
    "counter = 0\n",
    "try:\n",
    "    with open('loss_values.pkl', 'rb') as f_loss, open('accuracy_values.pkl', 'rb') as f_accuracy:\n",
    "        loss_array = pickle.load(f_loss)\n",
    "        accuracy_array = pickle.load(f_accuracy)\n",
    "except FileNotFoundError:\n",
    "    # If files don't exist, initialize empty arrays\n",
    "    loss_array = [] #used for graphing loss changes throughout training\n",
    "    accuracy_array = [] \n",
    "\n",
    "\n",
    "if start_epoch>0:\n",
    "    print(\"Found existing checkpoint, continue training\")\n",
    "    last_lost = resume(f'Model_at_epoch_{start_epoch}.pth')\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        curr_epoch = epoch\n",
    "        #Train\n",
    "        train_loss = trainloop(train_loader)\n",
    "\n",
    "        #Test\n",
    "        accuracy = evaluateloop(test_loader)\n",
    "        accuracy_array+=[accuracy]\n",
    "        loss_array+=[train_loss]\n",
    "\n",
    "        if counter%5==0:\n",
    "            checkpoint(model, optimizer, loss_array[-1], curr_epoch, f'Model_at_epoch_{curr_epoch}.pth')\n",
    "            with open('loss_values.pkl', 'wb') as f_loss, open('accuracy_values.pkl', 'wb') as f_accuracy:\n",
    "                pickle.dump(loss_array, f_loss)\n",
    "                pickle.dump(accuracy_array, f_accuracy)\n",
    "\n",
    "        counter+=1\n",
    "        # Print statistics at the end of each epoch\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] | Test Accuracy: {accuracy:.2%} | Current Loss: {train_loss: .6f}\\n')\n",
    "        #Early stopping check\n",
    "        if abs(train_loss - last_lost)<=delta:\n",
    "            trigger_times+=1\n",
    "        elif abs(train_loss - last_lost)>delta:\n",
    "            trigger_times=0\n",
    "        if trigger_times>patience:\n",
    "            stop = True\n",
    "        last_lost=train_loss\n",
    "        if stop == True:\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Saving current model checkpoint.\")\n",
    "    checkpoint(model, optimizer, loss_array[-1], curr_epoch, f'Model_at_epoch_{curr_epoch}.pth')\n",
    "    with open('loss_values.pkl', 'wb') as f_loss, open('accuracy_values.pkl', 'wb') as f_accuracy:\n",
    "        pickle.dump(loss_array, f_loss)\n",
    "        pickle.dump(accuracy_array, f_accuracy)\n",
    "\n",
    "# Print total training time\n",
    "print(\"Total training time: {:.2f} seconds\".format(time.time() - start_time))\n",
    "#Save model\n",
    "checkpoint(model, optimizer, loss_array[-1], curr_epoch, f'Model_at_epoch_{curr_epoch}.pth')\n",
    "with open('loss_values.pkl', 'wb') as f_loss, open('accuracy_values.pkl', 'wb') as f_accuracy:\n",
    "    pickle.dump(loss_array, f_loss)\n",
    "    pickle.dump(accuracy_array, f_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': curr_epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss_array[-1],\n",
    "}, f\"Model_at_epoch_{curr_epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'I love this game'\n",
    "test_sentence2 = 'Trash game'\n",
    "data = [test_sentence, test_sentence2,'Greate game i like it','I regret buying it']\n",
    "result = [1,0,1,0]\n",
    "column_names = ['text', 'positive']\n",
    "df = pd.DataFrame(list(zip(data, result)), columns=column_names)\n",
    "\n",
    "text = df['text']\n",
    "label = df['positive']\n",
    "evaluate = CustomDataset(text.apply(applyTransform),label.values)\n",
    "evaloader = DataLoader(evaluate,batch_size=1, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction result: I love this game - Correct\n",
      "Prediction result: Trash game - Incorrect\n",
      "Prediction result: Greate game i like it - Correct\n",
      "Prediction result: I regret buying it - Incorrect\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "for batch, text in zip(evaloader,data):\n",
    "    x,y,z= batch\n",
    "    predictions = model(x,y)\n",
    "#     print(predictions)\n",
    "    result = predictions.argmax(1)==z\n",
    "    if result == True:\n",
    "        print(f'Prediction result: {text} - Correct')\n",
    "    else:\n",
    "        print(f'Prediction result: {text} - Incorrect')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
