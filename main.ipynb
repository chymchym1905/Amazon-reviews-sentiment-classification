{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import gzip\n",
    "# import numpy as np\n",
    "# # import json\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Video_Games_Resampled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "      <th>Positive Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>346407</td>\n",
       "      <td>129412</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>05 3, 2010</td>\n",
       "      <td>A3DID3K8W8QD9W</td>\n",
       "      <td>B0012LHO46</td>\n",
       "      <td>Sam Hartford</td>\n",
       "      <td>bought mine ebay one amazon work fine generics...</td>\n",
       "      <td>PS2 64mb Memory Card is A-OK</td>\n",
       "      <td>1272844800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19478</td>\n",
       "      <td>204432</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>05 5, 2014</td>\n",
       "      <td>A1SPRD853KYUP1</td>\n",
       "      <td>B002I0K956</td>\n",
       "      <td>Oswaldo Romero</td>\n",
       "      <td>nice</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>1399248000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>349043</td>\n",
       "      <td>367307</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>01 4, 2017</td>\n",
       "      <td>A4SV916591F5Z</td>\n",
       "      <td>B00KY1I0IO</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>great fun addicting</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1483488000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'Format:': ' Video Game'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>238716</td>\n",
       "      <td>381621</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 12, 2016</td>\n",
       "      <td>AWXOF97NP6ZZG</td>\n",
       "      <td>B00O9GPD26</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>surprisingly higher quality figure american ve...</td>\n",
       "      <td>The figure is better than the american version.</td>\n",
       "      <td>1455235200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>416659</td>\n",
       "      <td>456490</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>08 5, 2005</td>\n",
       "      <td>A3LOOGDJ7S01LA</td>\n",
       "      <td>B00008J2V0</td>\n",
       "      <td>whiterabbit</td>\n",
       "      <td>highly recommended whole family easy instructi...</td>\n",
       "      <td>Lots of Fun, Lots to Explore!</td>\n",
       "      <td>1123200000</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  overall  verified   reviewTime      reviewerID  \\\n",
       "0        346407      129412      5.0     False   05 3, 2010  A3DID3K8W8QD9W   \n",
       "1         19478      204432      4.0      True   05 5, 2014  A1SPRD853KYUP1   \n",
       "2        349043      367307      5.0      True   01 4, 2017   A4SV916591F5Z   \n",
       "3        238716      381621      4.0      True  02 12, 2016   AWXOF97NP6ZZG   \n",
       "4        416659      456490      5.0     False   08 5, 2005  A3LOOGDJ7S01LA   \n",
       "\n",
       "         asin     reviewerName  \\\n",
       "0  B0012LHO46     Sam Hartford   \n",
       "1  B002I0K956   Oswaldo Romero   \n",
       "2  B00KY1I0IO  Amazon Customer   \n",
       "3  B00O9GPD26  Amazon Customer   \n",
       "4  B00008J2V0      whiterabbit   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  bought mine ebay one amazon work fine generics...   \n",
       "1                                               nice   \n",
       "2                                great fun addicting   \n",
       "3  surprisingly higher quality figure american ve...   \n",
       "4  highly recommended whole family easy instructi...   \n",
       "\n",
       "                                           summary  unixReviewTime vote  \\\n",
       "0                     PS2 64mb Memory Card is A-OK      1272844800  NaN   \n",
       "1                                       Four Stars      1399248000  NaN   \n",
       "2                                       Five Stars      1483488000  NaN   \n",
       "3  The figure is better than the american version.      1455235200  NaN   \n",
       "4                    Lots of Fun, Lots to Explore!      1123200000    7   \n",
       "\n",
       "                        style image  Positive Rating  \n",
       "0                         NaN   NaN                1  \n",
       "1                         NaN   NaN                1  \n",
       "2  {'Format:': ' Video Game'}   NaN                1  \n",
       "3                         NaN   NaN                1  \n",
       "4                         NaN   NaN                1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df.reindex(np.random.permutation(df.index))\n",
    "# ratingonly = df.shape\n",
    "# df = df.dropna(subset=['overall'],inplace=True)\n",
    "# df = df.dropna(subset=['reviewText'],inplace=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax=sns.countplot(x='overall', data=df)\n",
    "# ax.bar_label(ax.containers[0])\n",
    "# plt.grid(visible=True,axis='y',alpha=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating_threshold = 4\n",
    "# df['Positive Rating'] = df['overall'].apply(lambda x: 1 if x>=rating_threshold else 0)\n",
    "# df['Positive Rating'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# import re\n",
    "# from nltk.tokenize import word_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(text):\n",
    "#     # lower text\n",
    "#     text = text.lower()\n",
    "#     # remove hyperlinks\n",
    "#     text = re.compile(r'^https?://', re.IGNORECASE).sub(r'', text)\n",
    "#     # remove non-letters\n",
    "#     text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "#     text = word_tokenize(text)\n",
    "#     # remove stopwords\n",
    "#     stops = set(stopwords.words(\"english\"))\n",
    "#     text = [w for w in text if not w in stops]\n",
    "#     text = \" \".join(text)\n",
    "#     # remove punctuation\n",
    "#     text = RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "#     text = \" \".join(text)\n",
    "#     # remove words less than 3 letters\n",
    "#     text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "#     return text  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dropna(subset=['reviewText'],inplace=True)\n",
    "# df['reviewText'] = df['reviewText'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "\n",
    "# majority = df[df['Positive Rating']==1]\n",
    "# minority = df[df['Positive Rating']==0]\n",
    "# minority_upsampled = resample(minority, replace=True, n_samples=int(len(minority)*1.6), random_state=87)\n",
    "# majority_downsampled =  resample(majority, replace=True, n_samples=int(len(majority)/2.2), random_state=100)\n",
    "# resampled_df = pd.concat([majority_downsampled, minority_upsampled])\n",
    "\n",
    "# print(resampled_df.shape)\n",
    "# resampled_df['overall'].value_counts()\n",
    "# resampled_df.to_csv('Video_Games_Resampled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv('Video_Games_Resampled.csv')\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainx, testx, trainy, testy = train_test_split(df['reviewText'], df['Positive Rating'], test_size=0.23, random_state=42)\n",
    "print(len(trainx))\n",
    "print(len(testx))\n",
    "train = pd.concat([trainx, trainy], axis =1)\n",
    "test = pd.concat([testx, testy], axis=1)\n",
    "# train.to_csv('train_data.csv', index=True)\n",
    "# test.to_csv('test_data.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.transforms as T\n",
    "# spacy.cli.download(\"en_core_web_sm\")##newline\n",
    "eng = spacy.load(\"en_core_web_sm\")\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "def engTokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize an English text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in eng.tokenizer(text)]\n",
    "\n",
    "def tokenize(data):\n",
    "    for x in data:\n",
    "        yield engTokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_train = build_vocab_from_iterator(\n",
    "    tokenize(trainx),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "vocab_train.set_default_index(vocab_train['<unk>'])\n",
    "vocab_test = build_vocab_from_iterator(\n",
    "    tokenize(testx),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "vocab_test.set_default_index(vocab_test['<unk>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# if os.path.exists('vocab_train.pkl'):\n",
    "#     print('Load vocab train')\n",
    "#     with open('vocab_train.pkl', 'rb') as vocab_train_file:\n",
    "#         vocab_train = pickle.load(vocab_train_file)\n",
    "# else:\n",
    "#     # Save the trained vocab_train\n",
    "#     print('Save vocab train')\n",
    "#     with open('vocab_train.pkl', 'wb') as vocab_train_file:\n",
    "#         pickle.dump(vocab_train, vocab_train_file)\n",
    "\n",
    "# if os.path.exists('vocab_test.pkl'):\n",
    "#     print('Load vocab test')\n",
    "#     # Load vocab_test from the pickle file\n",
    "#     with open('vocab_test.pkl', 'rb') as vocab_test_file:\n",
    "#         vocab_test = pickle.load(vocab_test_file)\n",
    "# else:\n",
    "#     print('Save vocab test')\n",
    "#     # Save the trained vocab_test\n",
    "#     with open('vocab_test.pkl', 'wb') as vocab_test_file:\n",
    "#         pickle.dump(vocab_test, vocab_test_file)\n",
    "\n",
    "\n",
    "# print(vocab_train.get_itos()[:45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabsize = len(vocab_train)\n",
    "print(vocabsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions\n",
    "input_dim = vocabsize # The dimension of your input data (e.g., vocabulary size)\n",
    "hidden_dim = 256  # Size of the hidden layer\n",
    "embedding_dim =  128\n",
    "output_dim = 1  # Two classes: positive and negative\n",
    "learning_rate = 0.001\n",
    "epochs = 15\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classification model\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        # self.rnn = torch.nn.RNN(embedding_dim,\n",
    "        #                        hidden_dim,\n",
    "        #                        nonlinearity='relu')\n",
    "        self.rnn = torch.nn.LSTM(input_size= embedding_dim,\n",
    "                                 hidden_size=hidden_dim)        \n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text, length):\n",
    "        # text dim: [sentence length, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        # embedded dim: [sentence length, batch size, embedding dim]\n",
    "        # Pack the sequences for efficient processing\n",
    "        packed_data = pack_padded_sequence(embedded, length.sum(1), batch_first=True, enforce_sorted=False)\n",
    "        output, (hidden, cell) = self.rnn(packed_data)\n",
    "        # output dim: [sentence length, batch size, hidden dim]\n",
    "        # hidden dim: [1, batch size, hidden dim]\n",
    "        unpacked_output, unpacked_length = pad_packed_sequence(output,batch_first=True)\n",
    "        unpacked_output.squeeze_(1)\n",
    "        # hidden dim: [batch size, hidden dim]\n",
    "        o = self.fc(unpacked_output)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # sample = {'data': self.data[idx], 'label': self.labels[idx]}\n",
    "        return self.data.iloc[idx], self.labels[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = trainy.values\n",
    "y_test = testy.values\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTransform(vocab):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    text_tranform = T.Sequential(\n",
    "        ## converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "        # 2 as seen in previous section\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_tranform\n",
    "def applyTransform(text):\n",
    "    \"\"\"\n",
    "    Apply transforms to sequence of tokens in a sequence pair\n",
    "    \"\"\"\n",
    "\n",
    "    return getTransform(vocab_train)(engTokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = trainx.apply(applyTransform)\n",
    "temp = list(x_train)\n",
    "print(x_train[3])\n",
    "x_test = testx.apply(applyTransform)\n",
    "temp=list(x_test)\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available. Using CPU.\")\n",
    "print(device)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    data, labels = zip(*batch)\n",
    "\n",
    "    # Sort sequences by length (from longest to shortest)\n",
    "    sorted_data, sorted_labels = zip(*sorted(zip(data, labels), key=lambda x: len(x[0]), reverse=True))\n",
    "\n",
    "    # Pad sequences to the length of the longest sequence\n",
    "    padded_data = pad_sequence([torch.tensor(seq) for seq in sorted_data], batch_first=True)\n",
    "    padded_labels = torch.tensor(sorted_labels, dtype=torch.long)\n",
    "\n",
    "    # Create a mask for the padded elements\n",
    "    mask = (padded_data != 0).float()\n",
    "\n",
    "    return padded_data.to(device), mask, padded_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CustomDataset(x_train, y_train)\n",
    "testset = CustomDataset(x_test, y_test)\n",
    "\n",
    "# trainset = CustomDataset(vocab_train, y_train_onehot)\n",
    "# testset = CustomDataset(vocab_test, y_test)\n",
    "\n",
    "# Create a DataLoader for your dataset\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Move your model to the GPU\n",
    "\n",
    "# Instantiate the classifier\n",
    "model = SentimentClassifier(input_dim, embedding_dim, hidden_dim, output_dim).to(device)\n",
    "# Define the loss function (cross-entropy) and the optimizer (Adam)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1, 1507,  345,  ..., 1507,  345,    2],\n",
      "        [   1,  221, 1289,  ...,    0,    0,    0],\n",
      "        [   1,  103,    5,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   1,   11,    2,  ...,    0,    0,    0],\n",
      "        [   1,  299,    2,  ...,    0,    0,    0],\n",
      "        [   1,   48,    2,  ...,    0,    0,    0]], device='cuda:0')\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]], device='cuda:0')\n",
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n"
     ]
    }
   ],
   "source": [
    "for x,y,z in test_loader:\n",
    "    print(x)\n",
    "    print('AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')\n",
    "    print(y)\n",
    "    print('AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')\n",
    "    print(z.unsqueeze(1))\n",
    "    print('AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')\n",
    "    # seq_unpacked, lens_unpacked = pad_packed_sequence(x, batch_first=True)\n",
    "    # print(seq_unpacked)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True],\n",
      "        [True]], device='cuda:0')\n",
      "1.5625%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(54)\n",
    "total_acc, total_count = 0,0\n",
    "for x,y,z in test_loader:\n",
    "    z = z.unsqueeze(1)\n",
    "    predictions = model(x,y)\n",
    "    result = predictions.argmax(1)==z\n",
    "    result = result[torch.all(result, dim=1)]\n",
    "    print(result)\n",
    "    total_acc += len(result)\n",
    "    total_count += len(z)\n",
    "    print(f\"{total_acc/total_count*100}%\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2063 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [15:13<00:00,  2.26batch/s]\n",
      "100%|██████████| 2063/2063 [15:13<00:00,  2.26batch/s, loss=0.982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:47<00:00, 13.01batch/s, accuracy=0.482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15] | Test Accuracy: 48.18%\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [15:03<00:00,  2.28batch/s, loss=0.783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:48<00:00, 12.73batch/s, accuracy=0.559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15] | Test Accuracy: 55.86%\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [14:53<00:00,  2.31batch/s, loss=0.791] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:46<00:00, 13.22batch/s, accuracy=0.569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/15] | Test Accuracy: 56.86%\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [8:31:36<00:00, 14.88s/batch, loss=0.749]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:50<00:00, 12.11batch/s, accuracy=0.579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/15] | Test Accuracy: 57.89%\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [15:20<00:00,  2.24batch/s, loss=0.675] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:49<00:00, 12.34batch/s, accuracy=0.587]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/15] | Test Accuracy: 58.74%\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [15:01<00:00,  2.29batch/s, loss=0.729] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:46<00:00, 13.26batch/s, accuracy=0.591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/15] | Test Accuracy: 59.13%\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [14:57<00:00,  2.30batch/s, loss=0.633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:47<00:00, 12.89batch/s, accuracy=0.596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/15] | Test Accuracy: 59.56%\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [14:57<00:00,  2.30batch/s, loss=0.634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:48<00:00, 12.71batch/s, accuracy=0.6]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/15] | Test Accuracy: 59.98%\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [14:57<00:00,  2.30batch/s, loss=0.832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:48<00:00, 12.79batch/s, accuracy=0.605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/15] | Test Accuracy: 60.49%\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [15:03<00:00,  2.28batch/s, loss=0.735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:48<00:00, 12.69batch/s, accuracy=0.606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/15] | Test Accuracy: 60.59%\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [14:57<00:00,  2.30batch/s, loss=0.629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:47<00:00, 13.07batch/s, accuracy=0.607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/15] | Test Accuracy: 60.72%\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [14:49<00:00,  2.32batch/s, loss=0.565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:47<00:00, 13.11batch/s, accuracy=0.609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/15] | Test Accuracy: 60.89%\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [15:04<00:00,  2.28batch/s, loss=0.72] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:48<00:00, 12.77batch/s, accuracy=0.609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/15] | Test Accuracy: 60.89%\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [15:15<00:00,  2.25batch/s, loss=0.583] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:51<00:00, 12.05batch/s, accuracy=0.611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/15] | Test Accuracy: 61.08%\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2063/2063 [14:57<00:00,  2.30batch/s, loss=0.692] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 617/617 [00:46<00:00, 13.33batch/s, accuracy=0.61] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15] | Test Accuracy: 61.00%\n",
      "Total training time: 44053.39 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Wrap train_loader with tqdm\n",
    "with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Training\")\n",
    "        model.train()\n",
    "        # Wrap train_loader with tqdm to display progress bar\n",
    "        with tqdm(tepoch, unit=\"batch\") as tbatch:\n",
    "            for packed_data, mask, labels in tbatch:\n",
    "                labels = labels.unsqueeze(1)\n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(packed_data, mask)\n",
    "                loss = criterion(predictions, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tbatch.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Evaluation on the test set\n",
    "        print(\"Testing\")\n",
    "        model.eval()\n",
    "        total_acc, total_count = 0, 0\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Wrap test_loader with tqdm to display progress bar\n",
    "            with tqdm(test_loader, unit=\"batch\") as ttest:\n",
    "                for packed_data, mask, labels in ttest:\n",
    "                    labels = labels.unsqueeze(1)\n",
    "                    predictions = model(packed_data, mask)\n",
    "                    result = predictions.argmax(1)==labels\n",
    "                    result = result[torch.all(result, dim=1)]\n",
    "                    total_acc += len(result)\n",
    "                    total_count += len(labels)\n",
    "                    ttest.set_postfix(accuracy=total_acc / total_count)\n",
    "\n",
    "        accuracy = total_acc / total_count\n",
    "        print(f'Epoch [{epoch+1}/{epochs}] | Test Accuracy: {accuracy:.2%}')\n",
    "\n",
    "# Print total training time\n",
    "print(\"Total training time: {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'model M100M Quadro GPU 15 epoch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'I love this game'\n",
    "test_sentence2 = 'Trash game'\n",
    "data = [test_sentence, test_sentence2,'Greate game i like it','I regret buying it']\n",
    "result = [1,0,1,0]\n",
    "column_names = ['text', 'positive']\n",
    "df = pd.DataFrame(list(zip(data, result)), columns=column_names)\n",
    "\n",
    "text = df['text']\n",
    "label = df['positive']\n",
    "evaluate = CustomDataset(text.apply(applyTransform),label.values)\n",
    "evaloader = DataLoader(evaluate,batch_size=1, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction result: I love this game - Correct\n",
      "Prediction result: Trash game - Incorrect\n",
      "Prediction result: Greate game i like it - Correct\n",
      "Prediction result: I regret buying it - Incorrect\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "for batch, text in zip(evaloader,data):\n",
    "    x,y,z= batch\n",
    "    predictions = model(x,y)\n",
    "#     print(predictions)\n",
    "    result = predictions.argmax(1)==z\n",
    "    if result == True:\n",
    "        print(f'Prediction result: {text} - Correct')\n",
    "    else:\n",
    "        print(f'Prediction result: {text} - Incorrect')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
